{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WP7XTaNu8oIy"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Normal\n",
        "\n",
        "from os import path\n",
        "from typing import Optional\n",
        "from gym import spaces\n",
        "from gym.envs.classic_control import utils\n",
        "from gym.error import DependencyNotInstalled\n",
        "\n",
        "\n",
        "DEFAULT_X = np.pi\n",
        "DEFAULT_Y = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osHxYBIX89HB",
        "outputId": "1cad8a33-9de1-4bcf-921e-d78067c8085c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PendulumEnv(gym.Env):\n",
        "    \"\"\"\n",
        "       ### Description\n",
        "\n",
        "    The inverted pendulum swingup problem is based on the classic problem in control theory.\n",
        "    The system consists of a pendulum attached at one end to a fixed point, and the other end being free.\n",
        "    The pendulum starts in a random position and the goal is to apply torque on the free end to swing it\n",
        "    into an upright position, with its center of gravity right above the fixed point.\n",
        "\n",
        "    The diagram below specifies the coordinate system used for the implementation of the pendulum's\n",
        "    dynamic equations.\n",
        "\n",
        "    ![Pendulum Coordinate System](./diagrams/pendulum.png)\n",
        "\n",
        "    -  `x-y`: cartesian coordinates of the pendulum's end in meters.\n",
        "    - `theta` : angle in radians.\n",
        "    - `tau`: torque in `N m`. Defined as positive _counter-clockwise_.\n",
        "\n",
        "    ### Action Space\n",
        "\n",
        "    The action is a `ndarray` with shape `(1,)` representing the torque applied to free end of the pendulum.\n",
        "\n",
        "    | Num | Action | Min  | Max |\n",
        "    |-----|--------|------|-----|\n",
        "    | 0   | Torque | -2.0 | 2.0 |\n",
        "\n",
        "\n",
        "    ### Observation Space\n",
        "\n",
        "    The observation is a `ndarray` with shape `(3,)` representing the x-y coordinates of the pendulum's free\n",
        "    end and its angular velocity.\n",
        "\n",
        "    | Num | Observation      | Min  | Max |\n",
        "    |-----|------------------|------|-----|\n",
        "    | 0   | x = cos(theta)   | -1.0 | 1.0 |\n",
        "    | 1   | y = sin(theta)   | -1.0 | 1.0 |\n",
        "    | 2   | Angular Velocity | -8.0 | 8.0 |\n",
        "\n",
        "    ### Rewards\n",
        "\n",
        "    The reward function is defined as:\n",
        "\n",
        "    *r = -(theta<sup>2</sup> + 0.1 * theta_dt<sup>2</sup> + 0.001 * torque<sup>2</sup>)*\n",
        "\n",
        "    where `$\\theta$` is the pendulum's angle normalized between *[-pi, pi]* (with 0 being in the upright position).\n",
        "    Based on the above equation, the minimum reward that can be obtained is\n",
        "    *-(pi<sup>2</sup> + 0.1 * 8<sup>2</sup> + 0.001 * 2<sup>2</sup>) = -16.2736044*,\n",
        "    while the maximum reward is zero (pendulum is upright with zero velocity and no torque applied).\n",
        "\n",
        "    ### Starting State\n",
        "\n",
        "    The starting state is a random angle in *[-pi, pi]* and a random angular velocity in *[-1,1]*.\n",
        "\n",
        "    ### Episode Truncation\n",
        "\n",
        "    The episode truncates at 200 time steps.\n",
        "\n",
        "    ### Arguments\n",
        "\n",
        "    - `g`: acceleration of gravity measured in *(m s<sup>-2</sup>)* used to calculate the pendulum dynamics.\n",
        "      The default value is g = 10.0 .\n",
        "\n",
        "    ```\n",
        "    gym.make('Pendulum-v1', g=9.81)\n",
        "    ```\n",
        "\n",
        "    ### Version History\n",
        "\n",
        "    * v1: Simplify the math equations, no difference in behavior.\n",
        "    * v0: Initial versions release (1.0.0)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\n",
        "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
        "        \"render_fps\": 30,\n",
        "    }\n",
        "\n",
        "    def __init__(self, render_mode: Optional[str] = None, g=10.0):\n",
        "        self.max_speed = 8\n",
        "        self.max_torque = 2.0\n",
        "        self.dt = 0.05\n",
        "        self.g = g\n",
        "        self.m = 0.5\n",
        "        self.l = 1.0\n",
        "        self.b = 0.1\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.screen_dim = 500\n",
        "        self.screen = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "\n",
        "        high = np.array([1.0, 1.0, self.max_speed], dtype=np.float32)\n",
        "        # This will throw a warning in tests/envs/test_envs in utils/env_checker.py as the space is not symmetric\n",
        "        #   or normalised as max_torque == 2 by default. Ignoring the issue here as the default settings are too old\n",
        "        #   to update to follow the openai gym api\n",
        "        self.action_space = spaces.Box(\n",
        "            low=-self.max_torque, high=self.max_torque, shape=(1,), dtype=np.float32\n",
        "        )\n",
        "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
        "\n",
        "    def angle_normalize(self, x):\n",
        "      return ((x + np.pi) % (2 * np.pi)) - np.pi\n",
        "\n",
        "    def step(self, u):\n",
        "        th, thdot = self.state  # th := theta\n",
        "\n",
        "        g = self.g\n",
        "        m = self.m\n",
        "        l = self.l\n",
        "        dt = self.dt\n",
        "\n",
        "        u = np.clip(u, -self.max_torque, self.max_torque)\n",
        "        self.last_u = u  # for rendering\n",
        "        costs = self.angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)\n",
        "        # print(self.angle_normalize(th), u)\n",
        "        # print(costs)\n",
        "        newthdot = thdot + (3 * g / (2 * l) * np.sin(th) + 3.0 / (m * l**2) * u - self.b*thdot/(m*l**2)) * dt\n",
        "\n",
        "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
        "        newth = th + newthdot * dt\n",
        "\n",
        "        self.state = np.array([newth, newthdot])\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        \n",
        "        return self._get_obs(), -costs, False, False, {}\n",
        "\n",
        "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
        "        super().reset(seed=seed)\n",
        "        if options is None:\n",
        "            high = np.array([DEFAULT_X, DEFAULT_Y])\n",
        "        else:\n",
        "            # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
        "            # state/observations.\n",
        "            x = options.get(\"x_init\") if \"x_init\" in options else DEFAULT_X\n",
        "            y = options.get(\"y_init\") if \"y_init\" in options else DEFAULT_Y\n",
        "            x = utils.verify_number_and_cast(x)\n",
        "            y = utils.verify_number_and_cast(y)\n",
        "            high = np.array([x, y])\n",
        "        low = -high  # We enforce symmetric limits.\n",
        "        self.state = self.np_random.uniform(low=low, high=high)\n",
        "        self.last_u = None\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        theta, thetadot = self.state\n",
        "        return np.array([np.cos(theta), np.sin(theta), thetadot], dtype=np.float32)\n",
        "\n",
        "    def render(self):\n",
        "        # self.render_mode = 'human'\n",
        "        if self.render_mode is None:\n",
        "            gym.logger.warn(\n",
        "                \"You are calling render method without specifying any render mode. \"\n",
        "                \"You can specify the render_mode at initialization, \"\n",
        "                f'e.g. gym(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
        "            )\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            import pygame\n",
        "            from pygame import gfxdraw\n",
        "        except ImportError:\n",
        "            raise DependencyNotInstalled(\n",
        "                \"pygame is not installed, run `pip install gym[classic_control]`\"\n",
        "            )\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            if self.render_mode == \"human\":\n",
        "                pygame.display.init()\n",
        "                self.screen = pygame.display.set_mode(\n",
        "                    (self.screen_dim, self.screen_dim)\n",
        "                )\n",
        "            else:  # mode in \"rgb_array\"\n",
        "                self.screen = pygame.Surface((self.screen_dim, self.screen_dim))\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        self.surf = pygame.Surface((self.screen_dim, self.screen_dim))\n",
        "        self.surf.fill((255, 255, 255))\n",
        "\n",
        "        bound = 2.2\n",
        "        scale = self.screen_dim / (bound * 2)\n",
        "        offset = self.screen_dim // 2\n",
        "\n",
        "        rod_length = 1 * scale\n",
        "        rod_width = 0.2 * scale\n",
        "        l, r, t, b = 0, rod_length, rod_width / 2, -rod_width / 2\n",
        "        coords = [(l, b), (l, t), (r, t), (r, b)]\n",
        "        transformed_coords = []\n",
        "        for c in coords:\n",
        "            c = pygame.math.Vector2(c).rotate_rad(self.state[0] + np.pi / 2)\n",
        "            c = (c[0] + offset, c[1] + offset)\n",
        "            transformed_coords.append(c)\n",
        "        gfxdraw.aapolygon(self.surf, transformed_coords, (204, 77, 77))\n",
        "        gfxdraw.filled_polygon(self.surf, transformed_coords, (204, 77, 77))\n",
        "\n",
        "        gfxdraw.aacircle(self.surf, offset, offset, int(rod_width / 2), (204, 77, 77))\n",
        "        gfxdraw.filled_circle(\n",
        "            self.surf, offset, offset, int(rod_width / 2), (204, 77, 77)\n",
        "        )\n",
        "\n",
        "        rod_end = (rod_length, 0)\n",
        "        rod_end = pygame.math.Vector2(rod_end).rotate_rad(self.state[0] + np.pi / 2)\n",
        "        rod_end = (int(rod_end[0] + offset), int(rod_end[1] + offset))\n",
        "        gfxdraw.aacircle(\n",
        "            self.surf, rod_end[0], rod_end[1], int(rod_width / 2), (204, 77, 77)\n",
        "        )\n",
        "        gfxdraw.filled_circle(\n",
        "            self.surf, rod_end[0], rod_end[1], int(rod_width / 2), (204, 77, 77)\n",
        "        )\n",
        "\n",
        "        fname = path.join(path.dirname(__file__), \"assets/clockwise.png\")\n",
        "        img = pygame.image.load(fname)\n",
        "        if self.last_u is not None:\n",
        "            scale_img = pygame.transform.smoothscale(\n",
        "                img,\n",
        "                (scale * np.abs(self.last_u) / 2, scale * np.abs(self.last_u) / 2),\n",
        "            )\n",
        "            is_flip = bool(self.last_u > 0)\n",
        "            scale_img = pygame.transform.flip(scale_img, is_flip, True)\n",
        "            self.surf.blit(\n",
        "                scale_img,\n",
        "                (\n",
        "                    offset - scale_img.get_rect().centerx,\n",
        "                    offset - scale_img.get_rect().centery,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        # drawing axle\n",
        "        gfxdraw.aacircle(self.surf, offset, offset, int(0.05 * scale), (0, 0, 0))\n",
        "        gfxdraw.filled_circle(self.surf, offset, offset, int(0.05 * scale), (0, 0, 0))\n",
        "\n",
        "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
        "        self.screen.blit(self.surf, (0, 0))\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "            pygame.display.flip()\n",
        "\n",
        "        else:  # mode == \"rgb_array\":\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            import pygame\n",
        "\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.isopen = False\n"
      ],
      "metadata": {
        "id": "qJaTp0st-xVo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, obs_space_size, action_space_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.shared_layers = nn.Sequential(\n",
        "            nn.Linear(obs_space_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.policy_mean = nn.Sequential(\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_space_size)\n",
        "        )\n",
        "\n",
        "        self.policy_logstd = nn.Parameter(torch.zeros(1, action_space_size))\n",
        "\n",
        "        self.value_layers = nn.Sequential(\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def value(self, obs):\n",
        "        z = self.shared_layers(obs)\n",
        "        value = self.value_layers(z)\n",
        "        return value\n",
        "\n",
        "    def policy(self, obs):\n",
        "        z = self.shared_layers(obs)\n",
        "        mean = self.policy_mean(z)\n",
        "        std = torch.exp(self.policy_logstd).to(device)\n",
        "        return mean, std\n",
        "\n",
        "    def forward(self, obs):\n",
        "        z = self.shared_layers(obs)\n",
        "        mean = self.policy_mean(z)\n",
        "        std = torch.exp(self.policy_logstd).to(device)\n",
        "        value = self.value_layers(z)\n",
        "        return mean, std, value"
      ],
      "metadata": {
        "id": "cDDgHDCs9MBI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOTrainer:\n",
        "    def __init__(self, actor_critic, ppo_clip_val=0.2, target_kl_div=0.01,\n",
        "                 max_policy_train_iters=80, value_train_iters=80,\n",
        "                 policy_lr=3e-4, value_lr=1e-2):\n",
        "        self.ac = actor_critic\n",
        "        self.ppo_clip_val = ppo_clip_val\n",
        "        self.target_kl_div = target_kl_div\n",
        "        self.max_policy_train_iters = max_policy_train_iters\n",
        "        self.value_train_iters = value_train_iters\n",
        "\n",
        "        policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
        "            list(self.ac.policy_mean.parameters()) + \\\n",
        "            [self.ac.policy_logstd]\n",
        "        self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
        "\n",
        "        value_params = list(self.ac.shared_layers.parameters()) + \\\n",
        "            list(self.ac.value_layers.parameters())\n",
        "        self.value_optim = optim.Adam(value_params, lr=value_lr)\n",
        "\n",
        "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
        "        for _ in range(self.max_policy_train_iters):\n",
        "            self.policy_optim.zero_grad()\n",
        "\n",
        "            new_mean, new_std = self.ac.policy(obs)\n",
        "            dist = Normal(new_mean, new_std)\n",
        "            # dist_sample = dist.sample()\n",
        "            new_log_probs = dist.log_prob(acts).sum(dim=-1)\n",
        "\n",
        "            policy_ratio = torch.exp(new_log_probs - old_log_probs).to(device)\n",
        "            clipped_ratio = torch.clamp(policy_ratio, 1 - self.ppo_clip_val, 1 + self.ppo_clip_val).to(device)\n",
        "\n",
        "            clipped_loss = torch.min(clipped_ratio * gaes, policy_ratio * gaes).to(device)\n",
        "            policy_loss = -clipped_loss.mean()\n",
        "\n",
        "            policy_loss.backward()\n",
        "            self.policy_optim.step()\n",
        "\n",
        "            kl_div = (old_log_probs - new_log_probs).mean()\n",
        "            if kl_div >= self.target_kl_div:\n",
        "                break\n",
        "\n",
        "    def train_value(self, obs, returns):\n",
        "        for _ in range(self.value_train_iters):\n",
        "            self.value_optim.zero_grad()\n",
        "\n",
        "            values = self.ac.value(obs)\n",
        "            value_loss = (returns - values) ** 2\n",
        "            value_loss = value_loss.mean()\n",
        "\n",
        "            value_loss.backward()\n",
        "            self.value_optim.step()"
      ],
      "metadata": {
        "id": "aupgOEYY9Y7i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discount_rewards(rewards, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Return discounted rewards based on the given rewards and gamma param.\n",
        "    \"\"\"\n",
        "    new_rewards = [float(rewards[-1])]\n",
        "    for i in reversed(range(len(rewards)-1)):\n",
        "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
        "    return np.array(new_rewards[::-1])\n",
        "\n",
        "def calculate_gaes(rewards, values, gamma=0.99, decay=0.97):\n",
        "    \"\"\"\n",
        "    Return the General Advantage Estimates from the given rewards and values.\n",
        "    \"\"\"\n",
        "    next_values = np.concatenate([values[1:], [0]])\n",
        "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
        "\n",
        "    gaes = [deltas[-1]]\n",
        "    for i in reversed(range(len(deltas)-1)):\n",
        "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
        "\n",
        "    return np.array(gaes[::-1])"
      ],
      "metadata": {
        "id": "G5QzPB6d9b2x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout(model, env, max_steps=1000):\n",
        "    \"\"\"\n",
        "    Performs a single rollout.\n",
        "    Returns training data in the shape (n_steps, observation_shape)\n",
        "    and the cumulative reward.\n",
        "    \"\"\"\n",
        "\n",
        "    train_data = [[], [], [], [], []]  # obs, act, reward, values, act_log_probs\n",
        "    obs = env.reset()[0]\n",
        "\n",
        "    ep_reward = 0\n",
        "    for _ in range(max_steps):\n",
        "        mean, std, val = model(torch.tensor([obs], dtype=torch.float32, device=device))\n",
        "\n",
        "        act_distribution = MultivariateNormal(mean, torch.diag_embed(std).to(device))\n",
        "        act = act_distribution.sample()\n",
        "        act_log_prob = act_distribution.log_prob(act).item()\n",
        "\n",
        "        act, val = act.item(), val.item()\n",
        "        act = [act]\n",
        "\n",
        "        next_obs, reward, done, *_ = env.step(act)\n",
        "        next_obs = next_obs.ravel()\n",
        "        reward = reward[0]\n",
        "\n",
        "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
        "            train_data[i].append(item)\n",
        "\n",
        "        obs = next_obs\n",
        "        ep_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    train_data = [np.asarray(x) for x in train_data]\n",
        "\n",
        "    ### Do train data filtering\n",
        "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
        "\n",
        "    return train_data, ep_reward"
      ],
      "metadata": {
        "id": "Yf_ZvBg49gRK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = PendulumEnv()\n",
        "model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space.shape[0])\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "FoC6l1BJ9-Yj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training params\n",
        "n_episodes = 1000\n",
        "print_freq = 1\n",
        "\n",
        "ppo = PPOTrainer(\n",
        "    model,\n",
        "    policy_lr = 3e-4,\n",
        "    value_lr = 1e-3,\n",
        "    target_kl_div = 0.02,\n",
        "    max_policy_train_iters = 40,\n",
        "    value_train_iters = 40)"
      ],
      "metadata": {
        "id": "seQuoJTG-rBP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "ep_rewards = []\n",
        "for episode_idx in range(n_episodes):\n",
        "  # Perform rollout\n",
        "  train_data, reward = rollout(model, env)\n",
        "  ep_rewards.append(reward)\n",
        "\n",
        "  # Shuffle\n",
        "  permute_idxs = np.random.permutation(len(train_data[0]))\n",
        "\n",
        "  # Policy data\n",
        "  obs = torch.tensor(train_data[0][permute_idxs],\n",
        "                     dtype=torch.float32, device=device)\n",
        "  acts = torch.tensor(train_data[1][permute_idxs],\n",
        "                      dtype=torch.int32, device=device)\n",
        "  gaes = torch.tensor(train_data[3][permute_idxs],\n",
        "                      dtype=torch.float32, device=device)\n",
        "  act_log_probs = torch.tensor(train_data[4][permute_idxs],\n",
        "                               dtype=torch.float32, device=device)\n",
        "\n",
        "  # Value data\n",
        "  returns = discount_rewards(train_data[2])[permute_idxs]\n",
        "  returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
        "\n",
        "  # Train model\n",
        "  ppo.train_policy(obs, acts, act_log_probs, gaes)\n",
        "  ppo.train_value(obs, returns)\n",
        "\n",
        "  if (episode_idx + 1) % print_freq == 0:\n",
        "    print('Episode {} | Avg Reward {:.1f}'.format(\n",
        "        episode_idx + 1, np.mean(ep_rewards[-print_freq:])))"
      ],
      "metadata": {
        "id": "stdR-9h_-vkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fddf19b5-34f1-40df-833d-7cd375934940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-ce1416eeab90>:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  mean, std, val = model(torch.tensor([obs], dtype=torch.float32, device=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1 | Avg Reward -8248.3\n",
            "Episode 2 | Avg Reward -7723.0\n",
            "Episode 3 | Avg Reward -9341.9\n",
            "Episode 4 | Avg Reward -9359.7\n",
            "Episode 5 | Avg Reward -9394.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(len(ep_rewards)), ep_rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SDQl-WtJ_Kjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3gPg1L1rF67L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}